<!DOCTYPE html>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width">

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjs/5.2.3/math.js"></script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        "HTML-CSS" : {
            availableFonts : ["STIX"],
            preferredFont : "STIX",
            webFont : "STIX-Web",
            imageFont : null,
            scale : [70]
        }
    });
    </script>
<!-- <script src ="mathjax/MathJax.js?config=TeX-AMS_HTML-full"></script> -->
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
<!-- Font Awesome icons (free version)-->
<script src="https://use.fontawesome.com/releases/v5.13.0/js/all.js" crossorigin="anonymous"></script>


<!-- Google fonts-->
<link href="https://fonts.googleapis.com/css?family=Fira+Sans" rel="stylesheet">

<style>
h1 {
  color: #525368;
  font-size: 46px;
}

h2 {
  color: #7A7A7A;
}

h3 {
  color: #E26C22;
}

h4 {
  color: #1A4E66;
}

hr {
    background-color: #E26C22;
    color: #E26C22;
    border: 1px solid #E26C22;
}

section {
  background: #EFEFEF;
  color: #000;
  padding: 2em;
  /*font-family: "PT Serif", Baskerville, Georgia;*/
  font-family:  'Fira Sans'; 
  font-size: 30px;
}

#follow {
  background: none;
}

.grey {
  color: #777;
}

a:link,
a:visited {
  color: orange;
  text-decoration: none;
}

a:hover {
  text-decoration: underline;
}

.container {
    display: flex;
}
.col {
    flex: 1;
}

</style>

<section style="text-align: left; padding-top: 5em;">
    <h1>Bayesian Inference for Models</h2> 
    <h1> with Intractable Likelihood</h1> 
    <hr size=2 noshade>
    <h2>Sebastian Schmon, DPhil</h2>
    <h3>Department of Mathematical Sciences, Durham University</h3>
</section>

<section style="text-align: left; padding-top: 5em;">
    <h2>Part I</h2> 
    <hr size=2 noshade>
    <h3>Foundations of Simulation-based Inference</h3>
</section>

<section>
    <h2>Problem Setting</h2>
    <hr size=2 noshade>
    <h4>Intractable and Implicit Models</h4>

    <ul>
        <li>Statistical models are commonly defined through a likelihood function \(f(x \mid \theta)\)</li>
        <li>E.g. linear regression:
            $$
                y \sim \mathcal{N}(x^T\beta, \sigma^2).
            $$
        </li>
        <li>However, some models are very cumbersome or defined implicitely through the stochastic data generating mechanism (Diggle and Gratton, 1984). </li>
        <li>Examples: 
            <ul>
            <li>
                Simulators of complex systems.
            </li>
            <li>
                Population genetics/phylogentics.
            </li>
            <li>
                Social dynamics/economic models.
            </li>
            <li>
                Agent-based models are defined on an individual level.
            </li>    
            </ul>
            </li>
    </ul>

</section>

<section>
<h2>Example: Pandemic Modelling</h2>
        <div style="display: flex; justify-content: center;">
            <iframe width="900" height="500" src="https://www.youtube.com/embed/LfjbSrnJ00g" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
</section>

<section>
    <h2>Inverse Problems</h2>
    <hr size=2 noshade>
    <ul>
        <li>Often we are interested in finding the parameters that most likely produce certian outputs
        (inverse problem).</li>
        <li>
            Example:
            <ul>
                <li>Finding a virus' reproduction rate \(r_{0t}\)</li>
                <li>Finding coalescence time (time since the most
                    recent common ancestor, eg. Tavar√©, 1997)</li>
            </ul>
        </li>
    </ul>

</section>

<section>
    <h2>Bayesian Infernce</h2>
    <hr size=2 noshade>
    <h4>Basic Idea</h4>
    <ul>
        <li>Bayesians encode their <i>belief</i> about the model in a probability distribution \(p(\theta)\). </li>
        <li>Having observed data, Bayesians can <i>update</i> their beliefs by computing the conditional distribution</li>
        $$
            p(\theta \mid y) = \frac{{f(y \mid \theta)}p(\theta)}{f(y)} \propto {f(y \mid \theta)}p(\theta)
        $$
        with \(f(y) = \int {f(y \mid \theta)}p(\theta)d\theta\).
        <li>
            Example: \(r_{0t} \sim \log\mathcal{N}(0, \sigma^2)\) what is
            $$
                p(r_{0t} \leq 1 \mid \text{observed epidemic curve}).
            $$
        </li>
    </ul>
</section>

<section>
    <h2>Bayesian Infernce</h2>
    <hr size=2 noshade>
    <h4>Monte Carlo Principle</h4>
    <ul>
        <li><i>"All information about</i> \(p(\theta \mid y)\) <i>can be learned from samples</i> \(\theta \sim p(\theta \mid y)\)"</i></li>
        <li>The problem of computing \(p(\theta \mid y)\) is reduced to drawing samples, which is often easier.</li>
        <li>There are many Monte Carlo algorithms to sample from</li>
        $$
            p(\theta \mid y) \propto \color{grey}{f(y \mid \theta)}p(\theta).
        $$
        <li>But what can be done when \(\color{grey}{f(y \mid \theta)}\) cannot be computed?</li>
    </ul>
</section>

<section>
    <h2>Bayesian Infernce</h2>
    <hr size=2 noshade>
    <h4>(Approximate) Bayesian computation</h4>
    <ul>
        <li>
            Sample:
            $$
                \theta \sim p(\theta)
            $$
            then run simulator
            $$
                y' \sim f(y' \mid \theta).
            $$
            if \(y' = y_{obs}\) keep \(\theta\).
        </li>
        <li>
            The accepted samples of \(\theta\) are distributed according to \(p(\theta \mid y_{obs})\). 
        </li>
    </ul>
</section>

<section>
    <h2>Bayesian Infernce</h2>
    <hr size=2 noshade>
    <h4>Approximate Bayesian computation</h4>
    <ul>
        <li>
            Sample:
            $$
                \theta \sim p(\theta)
            $$
            then run simulator
            $$
                y' \sim f(y' \mid \theta).
            $$
            if \(d(y',y_{obs}) \leq \varepsilon\) keep \(\theta\).
        </li>
        <li>
            The accepted samples of \(\theta\) are distributed according to \(E[p(\theta \mid d(y',y_{obs}) \leq \varepsilon)]\). 
        </li>
    </ul>
</section>


<section>
    <h2>Bayesian Infernce</h2>
    <hr size=2 noshade>
    <h4>Approximate Bayesian computation: drawbacks</h4>
    <ol>
    <li>
            Not exact:
        <ul>
        <li>
            We only obtain samples from an approximation of the posterior. 
        </li>
        </ul>
    </li>
    <li> Not scalable:
        <ul>
            <li>
                Threshold parameter, \(\varepsilon\), usually needs to be very small.
            </li>
            <li>
                Requires many samples from \(y' \sim f(y' \mid \theta)\) to obtain a reasonable amount of samples from the approximate posterior.
            </li>
            <li>
                Problem: calling the simulator is often the most expensive part of the algorithm.
            </li>
        </ul>
    </li>
       
    </ol>
</section>

<section style="text-align: left; padding-top: 5em;">
    <h2>Part II</h2> 
    <hr size=2 noshade>
    <h3>Measurement Error and Model Inadequacy</h3>
</section>


<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    <ul>
        <li>
            In practice, the assumption \(y_{obs} \sim f( \cdot \mid \theta)\) if often unrealistic.
        </li>
        <li>
            E.g. an agent-based model of the Covid pandemic will not be able to reproduce the real data exactly because
            of model inadequacy and measurement error.
        </li>
        <li>
            Instead of \(f(y \mid \theta)\) "true" likelihood is given by
            $$
                p(y \mid \theta) = \int g(y \mid x,\theta) f(x \mid \theta)dx.
            $$
        </li>
    </ul>
</section>


<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    <ul>
        <li>
            Instead of \(f(y \mid \theta)\) "true" likelihood is given by
            $$
                p(y \mid \theta) = \int g(y \mid x,\theta) f(x \mid \theta)dx.
            $$
        </li>
        <li>
            Compare this to the ABC likelihood
            $$
                p_{ABC}(y \mid \theta) = \int 1_{d(s(x),s(y_{obs})) \leq \varepsilon} f(x \mid \theta)dx
            $$
        </li>
        <li>If \(g(y \mid x,\theta)\) is close to \(1_{d(s(x),s(y_{obs})) \leq \varepsilon}\), ABC is exact (unrealistic).</li>
    </ul>
</section>

<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    <ul>
        <li>
            Instead of \(f(y \mid \theta)\) "true" likelihood is given by
            $$
                p(y \mid \theta) = \int g(y \mid x,\theta) f(x \mid \theta)dx.
            $$
        </li>
        <li>
            Alternative
            $$
                p_{GABC}(y \mid \theta) = \int e^{-w \ell\left(y_{obs}, x,\right)} f(x \mid \theta)dx.
            $$
        </li>
        <li>Exact if \(g(y \mid x,\theta) \approx e^{-w \ell\left(y_{obs}, x,\right)}\) (Schmon et. al., 2020).</li>
        <li><i>Principled Bayesian belief update</i> following Bissiri et.al (2016).</li>
    </ul>
</section>

<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    <h4>Inference Algorithms: Pseudo-Marginal Methods (Andrieu et.al. 2010)</h4>
    Input: \(\theta, x\)
    <ul>
        <li> Sample new \(\theta' \sim q(\cdot \mid \theta)\) (usually with MCMC)
        <li> Sample \(x' \sim f(x' \mid \theta')\)
        <li> With probability 
            $$
                a(\theta, x; \theta', x') = \min\left\{1, \frac{e^{-w \ell\left(y_{obs}, x'\right)}}{e^{-w \ell\left(y_{obs}, x\right)}}\right\}
            $$
            accept (\(theta', y')\) otherwise keep \((\theta, x)\).
    </ul>
</section>


<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    Examples:
    <ul>
        <li> Soft-threshold ABC \(\ell(y, x) = \frac{1}{M} \sum_{i=1}^M d(s_{obs}, s_i)\)
        <li> Wasserstein ABC  \(\ell(y, x) = \frac{1}{M} \sum_{i=1}^M  W(y_{obs}, x_i)\)
        <li> K2-ABC  \(\ell(y, x) = \frac{1}{M} \sum_{i=1}^M  MMD(y_{obs}, x_i)\)
    </ul>
</section>

<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    <div style="display: flex; justify-content: center;">
        <img src="true_posterior.JPG" height=480 alt="coin_toss">
    </div>
</section>

<section>
    <h2>Robustness to Misspecification</h2>
    <hr size=2 noshade>
    <ul>
        <li> What happens for different loss functions when under various levels of model misspecification?
        <li> We study the following model: the "true" model is set to be 100 samples from \(y \sim \mathcal{N}(\theta=1, \sigma^2=2)\).
        <li> As a model we take the misspecified \(f(x \mid \theta) = \mathcal{N}(x; \theta, \sigma^2\in [0.5, 5])\) with prior beliefs \(p(\theta) = \mathcal{N}(\theta; 0, 25)\).
    </ul>
</section>

<section>
    <h2>Robustness to Misspecification</h2>
    <hr size=2 noshade>
    <div style="display: flex; justify-content: center;">
        <img src="comparison.JPG" height=480 alt="coin_toss">
    </div>
</section>

<section>
    <h2>Robustness to Misspecification</h2>
    <hr size=2 noshade>
    <h4>Conclusion</h4>
    <ul>
        <li>
            The "approximative" nature of ABC can be a feature, not a bug, when dealt with properly.
        </li>
        <lI>
            Soft-threshold ABC with general loss functions provide a princpled method for robust Bayesian inference.
        </lI>
        <li>
            Biggest unsolved issue: how to set \(w\).
        </li>
    </ul>
</section>


<section style="text-align: left; padding-top: 5em;">
    <h2>Part III</h2> 
    <hr size=2 noshade>
    <h3>Scalable Models for Simulation-based Inference</h3>
</section>

<section>
    <h2>The Frontier of Simulation-based Inference</h2>
    <hr size=2 noshade>
    <h4>(Cranmer et.al. 2020)</h4>
    <ul>
        <li>Modern machine learning techniques allow for flexible estimation of complex posteriors.
        <li>For example, given a sample of \((x_i, \theta_i) \sim p(x \mid \theta)p(\theta)\), we can use modern (conditional) density estimation techniques, such as <i>normalizing flows</i>
            or <i>Gaussian mixture models</i> to learn either
            $$
                p_\phi(\theta \mid x) \quad {\mathsf{(neural\, posterior\, estimation)}}
            $$
            or 
            $$
                p_\phi(x \mid \theta) \quad {\mathsf{(neural\, density\, estimation)}}.
            $$
    </ul>
</section>


<section>
    <h2>Neural Density Estimation</h2>
    <hr size=2 noshade>
    <h4>Normalizing Flows</h4>
    <ul>
        <li> Density transformation formula
            $$
                f_x(x) = f_u(T^{-1}(x)) |\det J_{T^{-1}}(x)| 
            $$
        <li> Choosing a base distribution \(f_u\), we can parameterize the transformation as \(T_\phi\) and thus learn a new density
            $$
                \log f_x(x) = \log f_u(T_\phi^{-1}(x)) + \log |\det J_{T_\phi^{-1}}(x)|.
            $$
        <li> This can be evaluated easily if the transformation is choosen well and optimized using automatic differentiation.
        <li> Examples: ResFlows, Masked Autoregressive Flows, Neural Spline Flows, Householder Flows, etc...
    </ul>
</section>


<section>
    <h2>Neural Density Estimation</h2>
    <hr size=2 noshade>
    <h4>(Sequential) Neural Likelihood (Papamakarios, 2018)</h4>
    Set \(r = 0, p_r(\theta \mid y_{obs}) = p(\theta)\) </br>
    For \(r = 1:R\)
    <ul>
        <li> Sample dataset \(x_i \sim f(x \mid \theta_i), \, \theta_i \sim p_{r-1}(\theta \mid y_{obs}), i=1, \ldots, N\)
        <li> Train \(q_\phi(x \mid \theta)\)
        <li> Set \(p_r(\theta\mid y_{obs}) = q_\phi(y_{obs} \mid \theta)p(\theta)\) 
    </ul>
</section>


<section>
    <h2>Neural Likelihood Estimation</h2>
    <hr size=2 noshade>
    <b>Advantages:</b>
    <li> Analytic expression for likelihood allows for traditional inference mechanisms (NUTS, HMC, slice sampling)
    <li> Because sampling from flows is easy (sample \(u\) and compute \(x = T(u)\)) we have a genreative surrogate model.
    <li> Often requires much fewer samples than ABC.
    </li>
    </br>
    <b>Disadvantages:</b>
    <li> Likelihood is still only an approximation: induces measurement error.
    </li> 
</section>

<section>
    <h2>Neural Likelihood Estimation: Model Inadequacy</h2>
    <hr size=2 noshade>
    <div style="display: flex; justify-content: center;">
        <img src="Slide1.PNG" height=480 alt="coin_toss">
    </div>
</section>


<section>
    <h2>Neural Likelihood Estimation</h2>
    <hr size=2 noshade>
    <h4>Solution (Schmon et. at. <i>forthcoming</i>)</h4>
    <li>Combine use the general ABC approach with neural likelihood.
    $$
        p(y \mid \theta) = \int e^{- w\ell(y_{obs}; x)} q_\phi(x \mid \theta)dx,
    $$
    where \(q_\phi(x \mid \theta) \approx f(x \mid \theta)\).
    </li>
</section>

<section>
    <h2>Neural Likelihood Estimation</h2>
    <hr size=2 noshade>
    <h4>So what?</h4>
    <li> Combines the advantages of neural density estimation and ABC.
    <li> Sampling is amortized since we only require samples from the flow surrogate instead of the simulator itself.
    <li> Can use recent advancements in pseudo-marginal methods, e.g. correlated pseudo-marginal (Deligiannidis et.al., 2018). 
    </li>
</section>


<section>
    <h2>Measurement Error and Model Inadequacy</h2>
    <hr size=2 noshade>
    <h4>Inference Algorithms: Pseudo-Marginal Methods (Andrieu et.al. 2010)</h4>
    Input: \(\(theta, u), x = T(u)\)
    <ul>
        <li> Sample new \(\theta' \sim q(\cdot \mid \theta)\) (usually with MCMC)
        <li> Set \(u' = \rho u + \sqrt{1 -\rho^2} \epsilon, \epsilon \sim \mathcal{N}(0, I)\)
        <li> Set \(x' = T(u')\)
        <li> With probability 
            $$
                a(\theta, x; \theta', x') = \min\left\{1, \frac{e^{-w \ell\left(y_{obs}, x'\right)}}{e^{-w \ell\left(y_{obs}, x\right)}}\right\}
            $$
            accept (\(theta', y')\) otherwise keep \((\theta, x)\).
    </ul>
</section>


<section>
    <h2>Neural Likelihood Estimation: Model Inadequacy</h2>
    <hr size=2 noshade>
    <div style="display: flex; justify-content: center;">
        <img src="pmh_flow_model_02-02-2021_14_44.png" height=480 alt="coin_toss">
    </div>
</section>

<section>
    <h2>Lotka-Volterra</h2>
    <hr size=2 noshade>
    <div style="display: flex; justify-content: center;">
        <img src="lotvol_mis_model.png" height=480 alt="coin_toss">
    </div>
</section>


<section>
  <h2>Summary</h2>
  <hr size=2 noshade>
  <ul>
    <li> We have shown that model inadequacy and measurement error can lead to considerable deviations from the desired posterior distribution.
    <li> Generalized ABC can help overcome these issues, in standard ABC and in particular in neural density estimation.
    </li>
  </ul>
</section>


<section style="text-align: left; padding-top: 5em;">
    <h2>Part IV</h2> 
    <hr size=2 noshade>
    <h3>Optimal Implementation of Inference Algorithms</h3>
</section>

<script src="d3.v3.min.js"></script>
<script src="stack.v1.min.js"></script>
<script>

var mystack = stack()
    .on("activate", activate)
    .on("deactivate", deactivate);

var section = d3.selectAll("section"),
    follow = d3.select("#follow"),
    followAnchor = d3.select("#follow-anchor"),
    lorenz = d3.select("#lorenz"),
    followIndex = section[0].indexOf(follow.node()),
    lorenzIndex = section[0].indexOf(lorenz.node()),
    time_comparison = d3.select("#time_comparison"),
    time_comparison_idx = section[0].indexOf(time_comparison.node())
    global_local_comparison = d3.select("#global_local_comparison"),
    global_local_comparison_idx = section[0].indexOf(global_local_comparison.node())
    ;

function refollow() {
  followAnchor.style("top", (followIndex + (1 - mystack.scrollRatio()) / 2 - d3.event.offset) * 100 + "%");
}

function activate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", refollow);
  //if (i === lorenzIndex) startLorenz();
  if (i === time_comparison_idx) startTimeComparison();
  if (i === global_local_comparison_idx) startGlobalLocalComparison();
}

function deactivate(d, i) {
  if (i === followIndex) mystack.on("scroll.follow", null);
  if (i === lorenzIndex) stopLorenz();
}

 
function startGlobalLocalComparison() {
    
    var canvas1 = d3.select("#global_canvas")
	.attr("width", width)
	.attr("height", height);    
    runLocalBps(global_bps_step, canvas1);
    
    var canvas2 = d3.select("#local_canvas")
	.attr("width", width)
	.attr("height", height);
    runLocalBps(local_bps_step, canvas2);   
}
  


// Standard D3.js scatterplot set up
  var margin = {top: 20, right: 20, bottom: 20, left: 20};
  var samples_width = 400 - margin.left - margin.right;
  var samples_height = 400 - margin.top  - margin.bottom;
  
  var estimate_width = 200 - margin.left - margin.right;
  var estimate_height = 200 - margin.left - margin.right;
  
  var svg_mh_samples = d3.select("#mh_graph").append("svg")
      .attr("height", samples_height + margin.left + margin.right)
      .attr("width", samples_width + margin.top + margin.bottom);
  
  var chart_mh_samples = svg_mh_samples.append("g")
      .attr("transform",
            "translate(" + margin.left + "," + margin.top + ")"
           );

  var xScale = d3.scale.linear()
      .domain([-3, 3])
      .range([0, samples_width]);
  
  var yScale = d3.scale.linear()
      .domain([-3, 3])
      .range([samples_height, 0]);
  
  var xAxis = d3.svg.axis()
      .scale(xScale)
      .orient("bottom")
      .tickSize(0)
      .tickFormat("");
  
  var yAxis = d3.svg.axis()
      .scale(yScale)
      .orient("left")
      .tickSize(0)
      .tickFormat("");
  
		




</script>
